#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sprint 4é«˜çº§åˆ†æç‰¹æ€§æµ‹è¯•ç”¨ä¾‹
æµ‹è¯•è­¦æŠ¥å¼•æ“å’ŒæŠ¥å‘Šç”ŸæˆåŠŸèƒ½
"""

import os
import sys
import tempfile
import asyncio

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from rules.alert_engine import GCAlertEngine, AlertRule
from analyzer.report_generator import GCReportGenerator, generate_gc_report
from main import generate_gc_report_tool


class MockMetrics:
    """æ¨¡æ‹ŸæŒ‡æ ‡å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰å¿…éœ€çš„å±æ€§"""
    def __init__(self):
        self.max_pause_time = 180.0
        self.min_pause_time = 15.0
        self.throughput_percentage = 88.5
        self.gc_frequency = 1.5
        self.full_gc_frequency = 0.0
        self.max_heap_utilization = 75.0
        self.memory_reclaim_efficiency = 45.2
        self.pause_time_trend = "increasing"
        self.memory_usage_trend = "stable"
        self.avg_pause_time = 73.5
        self.p99_pause_time = 175.0
        self.p50_pause_time = 65.0
        self.p95_pause_time = 165.0
        self.performance_score = 72.5
        self.gc_overhead_percentage = 11.5
        self.young_gc_frequency = 1.0
        self.avg_heap_utilization = 62.5
        self.memory_allocation_rate = 256.0
        self.health_status = "Warning"


class TestSprint4Features:
    """Sprint 4åŠŸèƒ½æµ‹è¯•ç±»"""
    
    def setup_method(self):
        """æµ‹è¯•å‰çš„è®¾ç½®"""
        self.alert_engine = GCAlertEngine()
        self.report_generator = GCReportGenerator()
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        self.test_events = [
            {
                'timestamp': 1.0,
                'gc_type': 'young',
                'pause_time': 15.5,
                'heap_before': 1024,
                'heap_after': 512,
                'heap_size': 2048
            },
            {
                'timestamp': 2.0,
                'gc_type': 'young',
                'pause_time': 180.0,  # è¿‡é•¿åœé¡¿
                'heap_before': 1536,
                'heap_after': 768,
                'heap_size': 2048
            },
            {
                'timestamp': 3.0,
                'gc_type': 'mixed',
                'pause_time': 25.0,
                'heap_before': 1280,
                'heap_after': 640,
                'heap_size': 2048
            }
        ]
        
        self.test_metrics = {
            'throughput': {
                'app_time_percentage': 88.5,
                'gc_time_percentage': 11.5
            },
            'latency': {
                'avg_pause_time': 73.5,
                'max_pause_time': 180.0,
                'p50_pause_time': 25.0,
                'p95_pause_time': 155.0,
                'p99_pause_time': 175.0,
                'min_pause_time': 15.5
            },
            'frequency': {
                'gc_frequency': 1.5,
                'young_gc_frequency': 1.0,
                'full_gc_frequency': 0.0
            },
            'memory': {
                'avg_heap_utilization': 62.5,
                'max_heap_utilization': 75.0,
                'memory_allocation_rate': 256.0,
                'memory_reclaim_efficiency': 45.2
            }
        }
        
        self.test_analysis = {
            'gc_type': 'G1 GC',
            'file_path': '/tmp/test.log',
            'total_events': 3
        }
    
    def test_alert_engine_basic(self):
        """æµ‹è¯•è­¦æŠ¥å¼•æ“åŸºç¡€åŠŸèƒ½"""
        print("ğŸ§ª æµ‹è¯•è­¦æŠ¥å¼•æ“åŸºç¡€åŠŸèƒ½...")
        
        mock_metrics = MockMetrics()
        
        # åˆ†ææŒ‡æ ‡
        alerts = self.alert_engine.evaluate_metrics(mock_metrics)
        
        # éªŒè¯è¿”å›çš„è­¦æŠ¥
        assert len(alerts) > 0, "åº”è¯¥æ£€æµ‹åˆ°è­¦æŠ¥"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é«˜åœé¡¿æ—¶é—´è­¦æŠ¥
        pause_alerts = [a for a in alerts if 'åœé¡¿æ—¶é—´' in a.message]
        assert len(pause_alerts) > 0, "åº”è¯¥æ£€æµ‹åˆ°åœé¡¿æ—¶é—´è­¦æŠ¥"
        
        print(f"âœ… æ£€æµ‹åˆ° {len(alerts)} ä¸ªè­¦æŠ¥")
        for alert in alerts:
            print(f"   - {alert.severity.value}: {alert.message}")
    
    def test_alert_engine_custom_rules(self):
        """æµ‹è¯•è­¦æŠ¥å¼•æ“è‡ªå®šä¹‰è§„åˆ™"""
        print("ğŸ§ª æµ‹è¯•è­¦æŠ¥å¼•æ“è‡ªå®šä¹‰è§„åˆ™...")
        
        from rules.alert_engine import AlertRule, AlertCategory, AlertSeverity
        
        # æ·»åŠ è‡ªå®šä¹‰è§„åˆ™
        custom_rule = AlertRule(
            name="test_memory_rule",
            description="å†…å­˜ä½¿ç”¨è¿‡é«˜æµ‹è¯•",
            category=AlertCategory.MEMORY,
            severity=AlertSeverity.WARNING,
            condition="max_heap_utilization > threshold",
            threshold=70.0,  # é™ä½é˜ˆå€¼ä»¥è§¦å‘è­¦æŠ¥
            message_template="è‡ªå®šä¹‰å†…å­˜è§„åˆ™è§¦å‘: {actual_value:.1f}%",
            recommendation="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•è§„åˆ™"
        )
        
        # åˆ›å»ºå¸¦è‡ªå®šä¹‰è§„åˆ™çš„å¼•æ“
        engine = GCAlertEngine()
        engine.rules.append(custom_rule)
        
        # åˆ›å»ºä¸€ä¸ªä½å †åˆ©ç”¨ç‡çš„Mockå¯¹è±¡
        class LowHeapMockMetrics(MockMetrics):
            def __init__(self):
                super().__init__()
                self.max_heap_utilization = 75.0  # è¶…è¿‡è‡ªå®šä¹‰é˜ˆå€¼70%
                self.max_pause_time = 50.0  # ä½äºé»˜è®¤é˜ˆå€¼
                self.throughput_percentage = 96.0  # é«˜äºé»˜è®¤é˜ˆå€¼
        
        mock_metrics = LowHeapMockMetrics()
        
        # åˆ†ææŒ‡æ ‡
        alerts = engine.evaluate_metrics(mock_metrics)
        
        # éªŒè¯è‡ªå®šä¹‰è§„åˆ™è§¦å‘
        memory_alerts = [a for a in alerts if 'è‡ªå®šä¹‰å†…å­˜è§„åˆ™' in a.message]
        assert len(memory_alerts) > 0, "è‡ªå®šä¹‰å†…å­˜è§„åˆ™åº”è¯¥è§¦å‘"
        
        print(f"âœ… è‡ªå®šä¹‰è§„åˆ™æˆåŠŸè§¦å‘")
    
    def test_report_generator_markdown(self):
        """æµ‹è¯•MarkdownæŠ¥å‘Šç”Ÿæˆ"""
        print("ğŸ§ª æµ‹è¯•MarkdownæŠ¥å‘Šç”Ÿæˆ...")
        
        mock_metrics = MockMetrics()
        
        # ç”Ÿæˆæµ‹è¯•è­¦æŠ¥
        alerts = self.alert_engine.evaluate_metrics(mock_metrics)
        alerts_data = [{
            'severity': alert.severity.value,
            'category': alert.category.value,
            'message': alert.message,
            'details': alert.recommendation
        } for alert in alerts]
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report = self.report_generator.generate_markdown_report(
            analysis_data=self.test_analysis,
            metrics_data=self.test_metrics,
            alerts_data=alerts_data
        )
        
        # éªŒè¯æŠ¥å‘Šå†…å®¹
        assert "# GCæ€§èƒ½åˆ†ææŠ¥å‘Š" in report, "åº”è¯¥åŒ…å«æŠ¥å‘Šæ ‡é¢˜"
        assert "G1 GC" in report, "åº”è¯¥åŒ…å«GCç±»å‹"
        assert "ğŸ“ˆ å…³é”®æ€§èƒ½æŒ‡æ ‡" in report, "åº”è¯¥åŒ…å«æ€§èƒ½æŒ‡æ ‡"
        assert "âš ï¸ æ€§èƒ½è­¦æŠ¥" in report, "åº”è¯¥åŒ…å«è­¦æŠ¥ä¿¡æ¯"
        assert "ğŸ’¡ ä¼˜åŒ–å»ºè®®" in report, "åº”è¯¥åŒ…å«ä¼˜åŒ–å»ºè®®"
        
        print("âœ… MarkdownæŠ¥å‘Šç”ŸæˆæˆåŠŸ")
        print(f"   æŠ¥å‘Šé•¿åº¦: {len(report)} å­—ç¬¦")
    
    def test_report_generator_html(self):
        """æµ‹è¯•HTMLæŠ¥å‘Šç”Ÿæˆ"""
        print("ğŸ§ª æµ‹è¯•HTMLæŠ¥å‘Šç”Ÿæˆ...")
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        report = self.report_generator.generate_html_report(
            analysis_data=self.test_analysis,
            metrics_data=self.test_metrics,
            alerts_data=[]
        )
        
        # éªŒè¯HTMLç»“æ„
        assert "<!DOCTYPE html>" in report, "åº”è¯¥åŒ…å«HTMLå£°æ˜"
        assert "<title>GCæ€§èƒ½åˆ†ææŠ¥å‘Š</title>" in report, "åº”è¯¥åŒ…å«æ ‡é¢˜"
        assert "ğŸ” GCæ€§èƒ½åˆ†ææŠ¥å‘Š" in report, "åº”è¯¥åŒ…å«æŠ¥å‘Šæ ‡é¢˜"
        assert "ğŸ“Š åŸºæœ¬ä¿¡æ¯" in report, "åº”è¯¥åŒ…å«åŸºæœ¬ä¿¡æ¯"
        assert "ğŸ“ˆ å…³é”®æ€§èƒ½æŒ‡æ ‡" in report, "åº”è¯¥åŒ…å«æ€§èƒ½æŒ‡æ ‡"
        
        print("âœ… HTMLæŠ¥å‘Šç”ŸæˆæˆåŠŸ")
        print(f"   æŠ¥å‘Šé•¿åº¦: {len(report)} å­—ç¬¦")
    
    def test_report_save_functionality(self):
        """æµ‹è¯•æŠ¥å‘Šä¿å­˜åŠŸèƒ½"""
        print("ğŸ§ª æµ‹è¯•æŠ¥å‘Šä¿å­˜åŠŸèƒ½...")
        
        with tempfile.TemporaryDirectory() as temp_dir:
            # æµ‹è¯•Markdownä¿å­˜
            md_file = os.path.join(temp_dir, "test_report")
            report = self.report_generator.generate_markdown_report(
                analysis_data=self.test_analysis,
                metrics_data=self.test_metrics
            )
            
            success = self.report_generator.save_report(report, md_file, "markdown")
            assert success, "MarkdownæŠ¥å‘Šä¿å­˜åº”è¯¥æˆåŠŸ"
            assert os.path.exists(md_file + ".md"), "Markdownæ–‡ä»¶åº”è¯¥å­˜åœ¨"
            
            # æµ‹è¯•HTMLä¿å­˜
            html_file = os.path.join(temp_dir, "test_report")
            html_report = self.report_generator.generate_html_report(
                analysis_data=self.test_analysis,
                metrics_data=self.test_metrics
            )
            
            success = self.report_generator.save_report(html_report, html_file, "html")
            assert success, "HTMLæŠ¥å‘Šä¿å­˜åº”è¯¥æˆåŠŸ"
            assert os.path.exists(html_file + ".html"), "HTMLæ–‡ä»¶åº”è¯¥å­˜åœ¨"
            
            print("âœ… æŠ¥å‘Šä¿å­˜åŠŸèƒ½æ­£å¸¸")
    
    def test_generate_gc_report_convenience_function(self):
        """æµ‹è¯•ä¾¿åˆ©å‡½æ•°"""
        print("ğŸ§ª æµ‹è¯•ä¾¿åˆ©å‡½æ•°...")
        
        # æµ‹è¯•Markdownç”Ÿæˆ
        md_report = generate_gc_report(
            analysis_data=self.test_analysis,
            metrics_data=self.test_metrics,
            format_type="markdown"
        )
        
        assert "# GCæ€§èƒ½åˆ†ææŠ¥å‘Š" in md_report, "ä¾¿åˆ©å‡½æ•°åº”è¯¥ç”Ÿæˆæ­£ç¡®çš„Markdown"
        
        # æµ‹è¯•HTMLç”Ÿæˆ
        html_report = generate_gc_report(
            analysis_data=self.test_analysis,
            metrics_data=self.test_metrics,
            format_type="html"
        )
        
        assert "<!DOCTYPE html>" in html_report, "ä¾¿åˆ©å‡½æ•°åº”è¯¥ç”Ÿæˆæ­£ç¡®çš„HTML"
        
        print("âœ… ä¾¿åˆ©å‡½æ•°å·¥ä½œæ­£å¸¸")
    
    def test_mcp_generate_report_tool(self):
        """æµ‹è¯•MCPæŠ¥å‘Šç”Ÿæˆå·¥å…·ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        print("ğŸ§ª æµ‹è¯•MCPæŠ¥å‘Šç”Ÿæˆå·¥å…·...")
        
        # ç”±äºéœ€è¦å…¨å±€çŠ¶æ€ï¼Œè¿™é‡Œåªæµ‹è¯•å‚æ•°éªŒè¯
        from main import current_analysis_result
        
        # æ¨¡æ‹Ÿè®¾ç½®å…¨å±€çŠ¶æ€
        original_result = current_analysis_result
        
        try:
            # æµ‹è¯•æ— åˆ†æç»“æœçš„æƒ…å†µ
            import main
            main.current_analysis_result = None
            
            async def test_no_analysis():
                try:
                    await generate_gc_report_tool({})
                    assert False, "åº”è¯¥æŠ›å‡ºé”™è¯¯"
                except ValueError as e:
                    assert "è¯·å…ˆä½¿ç”¨analyze_gc_logå·¥å…·" in str(e)
                    return True
                return False
            
            result = asyncio.run(test_no_analysis())
            assert result, "æ— åˆ†æç»“æœåº”è¯¥æŠ›å‡ºæ­£ç¡®é”™è¯¯"
            
            print("âœ… MCPå·¥å…·å‚æ•°éªŒè¯æ­£å¸¸")
            
        finally:
            # æ¢å¤åŸå§‹çŠ¶æ€
            import main
            main.current_analysis_result = original_result
    
    def test_integration_workflow(self):
        """æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹"""
        print("ğŸ§ª æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹...")
        
        mock_metrics = MockMetrics()
        
        # 1. è­¦æŠ¥åˆ†æ
        alerts = self.alert_engine.evaluate_metrics(mock_metrics)
        assert len(alerts) > 0, "åº”è¯¥ç”Ÿæˆè­¦æŠ¥"
        
        # 2. è½¬æ¢è­¦æŠ¥æ•°æ®
        alerts_data = [{
            'severity': alert.severity.value,
            'category': alert.category.value,
            'message': alert.message,
            'details': alert.recommendation
        } for alert in alerts]
        
        # 3. ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
        report = generate_gc_report(
            analysis_data=self.test_analysis,
            metrics_data=self.test_metrics,
            alerts_data=alerts_data,
            format_type="markdown"
        )
        
        # éªŒè¯å®Œæ•´æŠ¥å‘Š
        assert "# GCæ€§èƒ½åˆ†ææŠ¥å‘Š" in report
        assert "ğŸ“ˆ å…³é”®æ€§èƒ½æŒ‡æ ‡" in report
        assert "âš ï¸ æ€§èƒ½è­¦æŠ¥" in report
        assert "ğŸ’¡ ä¼˜åŒ–å»ºè®®" in report
        
        # éªŒè¯è­¦æŠ¥å†…å®¹è¢«åŒ…å«
        assert any(alert['message'] in report for alert in alerts_data), "è­¦æŠ¥ä¿¡æ¯åº”è¯¥åœ¨æŠ¥å‘Šä¸­"
        
        print("âœ… å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡")


def run_sprint4_tests():
    """è¿è¡ŒSprint 4çš„æ‰€æœ‰æµ‹è¯•"""
    test_instance = TestSprint4Features()
    test_instance.setup_method()
    
    tests = [
        ("è­¦æŠ¥å¼•æ“åŸºç¡€åŠŸèƒ½", test_instance.test_alert_engine_basic),
        ("è­¦æŠ¥å¼•æ“è‡ªå®šä¹‰è§„åˆ™", test_instance.test_alert_engine_custom_rules),
        ("MarkdownæŠ¥å‘Šç”Ÿæˆ", test_instance.test_report_generator_markdown),
        ("HTMLæŠ¥å‘Šç”Ÿæˆ", test_instance.test_report_generator_html),
        ("æŠ¥å‘Šä¿å­˜åŠŸèƒ½", test_instance.test_report_save_functionality),
        ("ä¾¿åˆ©å‡½æ•°", test_instance.test_generate_gc_report_convenience_function),
        ("MCPå·¥å…·å‚æ•°éªŒè¯", test_instance.test_mcp_generate_report_tool),
        ("å®Œæ•´å·¥ä½œæµç¨‹", test_instance.test_integration_workflow),
    ]
    
    passed = 0
    total = len(tests)
    
    print("ğŸš€ å¼€å§‹Sprint 4é«˜çº§åˆ†æç‰¹æ€§æµ‹è¯•...\n")
    
    for test_name, test_func in tests:
        print(f"ğŸ§ª è¿è¡Œæµ‹è¯•: {test_name}")
        try:
            test_func()
            passed += 1
            print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡\n")
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥: {e}\n")
    
    print(f"ğŸ“Š Sprint 4æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ Sprint 4æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return True
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥é—®é¢˜")
        return False


if __name__ == "__main__":
    success = run_sprint4_tests()
    sys.exit(0 if success else 1)