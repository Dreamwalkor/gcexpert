#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sprint 5: æ€§èƒ½æµ‹è¯•ä¸ä¼˜åŒ–
éªŒè¯GCæ—¥å¿—åˆ†æç³»ç»Ÿçš„æ€§èƒ½æŒ‡æ ‡
"""

import os
import sys
import time
import asyncio
from statistics import mean, median

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from main import (
    analyze_gc_log_tool,
    get_gc_metrics_tool,
    detect_gc_issues_tool,
    generate_gc_report_tool
)


class PerformanceTest:
    """æ€§èƒ½æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.test_data_dir = os.path.join(project_root, 'test', 'data')
        self.sample_g1_log = os.path.join(self.test_data_dir, 'sample_g1.log')
        self.sample_j9_log = os.path.join(self.test_data_dir, 'sample_j9.log')
        self.performance_results = {}
    
    async def measure_function_time(self, func, *args, **kwargs):
        """æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´"""
        start_time = time.time()
        result = await func(*args, **kwargs)
        end_time = time.time()
        execution_time = end_time - start_time
        return result, execution_time
    
    async def test_analyze_performance(self):
        """æµ‹è¯•åˆ†æåŠŸèƒ½çš„æ€§èƒ½"""
        print("âš¡ æµ‹è¯•GCæ—¥å¿—åˆ†ææ€§èƒ½...")
        
        if not os.path.exists(self.sample_g1_log):
            print("âš ï¸ è·³è¿‡æµ‹è¯•ï¼šG1æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        # å¤šæ¬¡è¿è¡Œæµ‹è¯•è·å–å¹³å‡æ€§èƒ½
        times = []
        test_count = 5
        
        for i in range(test_count):
            result, exec_time = await self.measure_function_time(
                analyze_gc_log_tool,
                {"file_path": self.sample_g1_log, "analysis_type": "detailed"}
            )
            times.append(exec_time)
            print(f"   è¿è¡Œ {i+1}: {exec_time:.3f}s")
        
        avg_time = mean(times)
        median_time = median(times)
        min_time = min(times)
        max_time = max(times)
        
        self.performance_results['analyze'] = {
            'avg_time': avg_time,
            'median_time': median_time,
            'min_time': min_time,
            'max_time': max_time
        }
        
        print(f"âœ… åˆ†ææ€§èƒ½ç»Ÿè®¡:")
        print(f"   å¹³å‡æ—¶é—´: {avg_time:.3f}s")
        print(f"   ä¸­ä½æ•°æ—¶é—´: {median_time:.3f}s")
        print(f"   æœ€å¿«æ—¶é—´: {min_time:.3f}s")
        print(f"   æœ€æ…¢æ—¶é—´: {max_time:.3f}s")
        
        # æ€§èƒ½æ ‡å‡†ï¼šåˆ†æåº”è¯¥åœ¨2ç§’å†…å®Œæˆ
        if avg_time < 2.0:
            print("ğŸš€ åˆ†ææ€§èƒ½ä¼˜ç§€!")
            return True
        elif avg_time < 5.0:
            print("âœ… åˆ†ææ€§èƒ½è‰¯å¥½")
            return True
        else:
            print("âš ï¸ åˆ†ææ€§èƒ½éœ€è¦ä¼˜åŒ–")
            return False
    
    async def test_metrics_performance(self):
        """æµ‹è¯•æŒ‡æ ‡è·å–æ€§èƒ½"""
        print("ğŸ“Š æµ‹è¯•æŒ‡æ ‡è·å–æ€§èƒ½...")
        
        if not os.path.exists(self.sample_g1_log):
            print("âš ï¸ è·³è¿‡æµ‹è¯•ï¼šéœ€è¦å…ˆåˆ†ææ—¥å¿—")
            return True
        
        # å…ˆåˆ†æä¸€ä¸ªæ—¥å¿—
        await analyze_gc_log_tool({
            "file_path": self.sample_g1_log,
            "analysis_type": "detailed"
        })
        
        # æµ‹è¯•æŒ‡æ ‡è·å–æ€§èƒ½
        times = []
        test_count = 10
        
        for i in range(test_count):
            result, exec_time = await self.measure_function_time(
                get_gc_metrics_tool,
                {"metric_types": ["all"]}
            )
            times.append(exec_time)
        
        avg_time = mean(times)
        min_time = min(times)
        max_time = max(times)
        
        self.performance_results['metrics'] = {
            'avg_time': avg_time,
            'min_time': min_time,
            'max_time': max_time
        }
        
        print(f"âœ… æŒ‡æ ‡è·å–æ€§èƒ½:")
        print(f"   å¹³å‡æ—¶é—´: {avg_time:.3f}s")
        print(f"   æœ€å¿«æ—¶é—´: {min_time:.3f}s")
        print(f"   æœ€æ…¢æ—¶é—´: {max_time:.3f}s")
        
        # æŒ‡æ ‡è·å–åº”è¯¥éå¸¸å¿«ï¼ˆå°äº0.1ç§’ï¼‰
        if avg_time < 0.1:
            print("ğŸš€ æŒ‡æ ‡è·å–æ€§èƒ½ä¼˜ç§€!")
            return True
        elif avg_time < 0.5:
            print("âœ… æŒ‡æ ‡è·å–æ€§èƒ½è‰¯å¥½")
            return True
        else:
            print("âš ï¸ æŒ‡æ ‡è·å–æ€§èƒ½éœ€è¦ä¼˜åŒ–")
            return False
    
    async def test_report_generation_performance(self):
        """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆæ€§èƒ½"""
        print("ğŸ“ æµ‹è¯•æŠ¥å‘Šç”Ÿæˆæ€§èƒ½...")
        
        if not os.path.exists(self.sample_g1_log):
            print("âš ï¸ è·³è¿‡æµ‹è¯•ï¼šéœ€è¦å…ˆåˆ†ææ—¥å¿—")
            return True
        
        # å…ˆåˆ†æä¸€ä¸ªæ—¥å¿—
        await analyze_gc_log_tool({
            "file_path": self.sample_g1_log,
            "analysis_type": "detailed"
        })
        
        # æµ‹è¯•MarkdownæŠ¥å‘Šç”Ÿæˆ
        md_result, md_time = await self.measure_function_time(
            generate_gc_report_tool,
            {"format_type": "markdown", "include_alerts": True}
        )
        
        # æµ‹è¯•HTMLæŠ¥å‘Šç”Ÿæˆ
        html_result, html_time = await self.measure_function_time(
            generate_gc_report_tool,
            {"format_type": "html", "include_alerts": True}
        )
        
        self.performance_results['reports'] = {
            'markdown_time': md_time,
            'html_time': html_time
        }
        
        print(f"âœ… æŠ¥å‘Šç”Ÿæˆæ€§èƒ½:")
        print(f"   MarkdownæŠ¥å‘Š: {md_time:.3f}s")
        print(f"   HTMLæŠ¥å‘Š: {html_time:.3f}s")
        
        # æŠ¥å‘Šç”Ÿæˆåº”è¯¥åœ¨1ç§’å†…å®Œæˆ
        if md_time < 1.0 and html_time < 1.0:
            print("ğŸš€ æŠ¥å‘Šç”Ÿæˆæ€§èƒ½ä¼˜ç§€!")
            return True
        elif md_time < 2.0 and html_time < 2.0:
            print("âœ… æŠ¥å‘Šç”Ÿæˆæ€§èƒ½è‰¯å¥½")
            return True
        else:
            print("âš ï¸ æŠ¥å‘Šç”Ÿæˆæ€§èƒ½éœ€è¦ä¼˜åŒ–")
            return False
    
    async def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        print("ğŸ’¾ æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ...")
        
        try:
            import psutil
            import os
            
            # è·å–å½“å‰è¿›ç¨‹
            process = psutil.Process(os.getpid())
            
            # è®°å½•åˆå§‹å†…å­˜ä½¿ç”¨
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            if os.path.exists(self.sample_g1_log):
                # æ‰§è¡Œä¸€ç³»åˆ—æ“ä½œ
                await analyze_gc_log_tool({
                    "file_path": self.sample_g1_log,
                    "analysis_type": "detailed"
                })
                
                await get_gc_metrics_tool({"metric_types": ["all"]})
                
                await detect_gc_issues_tool({})
                
                await generate_gc_report_tool({
                    "format_type": "markdown",
                    "include_alerts": True
                })
            
            # è®°å½•æœ€ç»ˆå†…å­˜ä½¿ç”¨
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = final_memory - initial_memory
            
            print(f"âœ… å†…å­˜ä½¿ç”¨æƒ…å†µ:")
            print(f"   åˆå§‹å†…å­˜: {initial_memory:.1f}MB")
            print(f"   æœ€ç»ˆå†…å­˜: {final_memory:.1f}MB")
            print(f"   å†…å­˜å¢åŠ : {memory_increase:.1f}MB")
            
            # å†…å­˜å¢åŠ åº”è¯¥å°äº50MB
            if memory_increase < 50:
                print("ğŸš€ å†…å­˜ä½¿ç”¨ä¼˜ç§€!")
                return True
            elif memory_increase < 100:
                print("âœ… å†…å­˜ä½¿ç”¨è‰¯å¥½")
                return True
            else:
                print("âš ï¸ å†…å­˜ä½¿ç”¨éœ€è¦ä¼˜åŒ–")
                return False
                
        except ImportError:
            print("âš ï¸ è·³è¿‡å†…å­˜æµ‹è¯•ï¼šç¼ºå°‘psutilåº“")
            return True
    
    async def test_concurrent_performance(self):
        """æµ‹è¯•å¹¶å‘æ€§èƒ½"""
        print("ğŸ”„ æµ‹è¯•å¹¶å‘å¤„ç†æ€§èƒ½...")
        
        if not os.path.exists(self.sample_g1_log):
            print("âš ï¸ è·³è¿‡æµ‹è¯•ï¼šéœ€è¦æµ‹è¯•æ•°æ®æ–‡ä»¶")
            return True
        
        # å¹¶å‘æ‰§è¡Œå¤šä¸ªåˆ†æä»»åŠ¡
        concurrent_count = 3
        
        start_time = time.time()
        
        tasks = []
        for i in range(concurrent_count):
            task = analyze_gc_log_tool({
                "file_path": self.sample_g1_log,
                "analysis_type": "basic"
            })
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # éªŒè¯æ‰€æœ‰ä»»åŠ¡éƒ½æˆåŠŸå®Œæˆ
        success_count = len([r for r in results if r.content])
        
        print(f"âœ… å¹¶å‘æ€§èƒ½æµ‹è¯•:")
        print(f"   ä»»åŠ¡æ•°é‡: {concurrent_count}")
        print(f"   æˆåŠŸå®Œæˆ: {success_count}")
        print(f"   æ€»æ—¶é—´: {total_time:.3f}s")
        print(f"   å¹³å‡æ¯ä»»åŠ¡: {total_time/concurrent_count:.3f}s")
        
        if success_count == concurrent_count and total_time < 10.0:
            print("ğŸš€ å¹¶å‘æ€§èƒ½ä¼˜ç§€!")
            return True
        elif success_count == concurrent_count:
            print("âœ… å¹¶å‘æ€§èƒ½è‰¯å¥½")
            return True
        else:
            print("âš ï¸ å¹¶å‘æ€§èƒ½éœ€è¦ä¼˜åŒ–")
            return False
    
    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        print("\nğŸ“Š æ€§èƒ½æµ‹è¯•æŠ¥å‘Š:")
        print("=" * 50)
        
        if 'analyze' in self.performance_results:
            analyze = self.performance_results['analyze']
            print(f"ğŸ” æ—¥å¿—åˆ†ææ€§èƒ½:")
            print(f"   å¹³å‡æ—¶é—´: {analyze['avg_time']:.3f}s")
            print(f"   æ€§èƒ½è¯„çº§: {'ä¼˜ç§€' if analyze['avg_time'] < 2.0 else 'è‰¯å¥½' if analyze['avg_time'] < 5.0 else 'éœ€ä¼˜åŒ–'}")
        
        if 'metrics' in self.performance_results:
            metrics = self.performance_results['metrics']
            print(f"ğŸ“ˆ æŒ‡æ ‡è·å–æ€§èƒ½:")
            print(f"   å¹³å‡æ—¶é—´: {metrics['avg_time']:.3f}s")
            print(f"   æ€§èƒ½è¯„çº§: {'ä¼˜ç§€' if metrics['avg_time'] < 0.1 else 'è‰¯å¥½' if metrics['avg_time'] < 0.5 else 'éœ€ä¼˜åŒ–'}")
        
        if 'reports' in self.performance_results:
            reports = self.performance_results['reports']
            print(f"ğŸ“ æŠ¥å‘Šç”Ÿæˆæ€§èƒ½:")
            print(f"   Markdown: {reports['markdown_time']:.3f}s")
            print(f"   HTML: {reports['html_time']:.3f}s")
            md_rating = 'ä¼˜ç§€' if reports['markdown_time'] < 1.0 else 'è‰¯å¥½' if reports['markdown_time'] < 2.0 else 'éœ€ä¼˜åŒ–'
            html_rating = 'ä¼˜ç§€' if reports['html_time'] < 1.0 else 'è‰¯å¥½' if reports['html_time'] < 2.0 else 'éœ€ä¼˜åŒ–'
            print(f"   æ€§èƒ½è¯„çº§: Markdown-{md_rating}, HTML-{html_rating}")
        
        print("=" * 50)


async def run_performance_tests():
    """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
    perf_test = PerformanceTest()
    
    tests = [
        ("æ—¥å¿—åˆ†ææ€§èƒ½", perf_test.test_analyze_performance),
        ("æŒ‡æ ‡è·å–æ€§èƒ½", perf_test.test_metrics_performance),
        ("æŠ¥å‘Šç”Ÿæˆæ€§èƒ½", perf_test.test_report_generation_performance),
        ("å†…å­˜ä½¿ç”¨æµ‹è¯•", perf_test.test_memory_usage),
        ("å¹¶å‘å¤„ç†æ€§èƒ½", perf_test.test_concurrent_performance),
    ]
    
    passed = 0
    total = len(tests)
    
    print("ğŸš€ å¼€å§‹Sprint 5æ€§èƒ½æµ‹è¯•...\n")
    
    for test_name, test_func in tests:
        print(f"ğŸ§ª è¿è¡Œæµ‹è¯•: {test_name}")
        try:
            result = await test_func()
            if result:
                passed += 1
                print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡\n")
            else:
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥\n")
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}\n")
    
    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    perf_test.generate_performance_report()
    
    print(f"\nğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed >= total * 0.8:  # 80%é€šè¿‡ç‡å³ä¸ºè‰¯å¥½
        print("ğŸ‰ ç³»ç»Ÿæ€§èƒ½è¡¨ç°è‰¯å¥½ï¼")
        return True
    else:
        print("âš ï¸ ç³»ç»Ÿæ€§èƒ½éœ€è¦ä¼˜åŒ–")
        return False


if __name__ == "__main__":
    success = asyncio.run(run_performance_tests())
    sys.exit(0 if success else 1)