#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sprint 5: ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•
éªŒè¯å®Œæ•´çš„GCæ—¥å¿—åˆ†æç³»ç»ŸåŠŸèƒ½
"""

import os
import sys
import asyncio
import tempfile

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from main import (
    analyze_gc_log_tool,
    get_gc_metrics_tool,
    compare_gc_logs_tool,
    detect_gc_issues_tool,
    generate_gc_report_tool,
    list_tools
)


class TestEndToEndIntegration:
    """ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•ç±»"""
    
    def setup_method(self):
        """æµ‹è¯•å‰çš„è®¾ç½®"""
        self.test_data_dir = os.path.join(project_root, 'test', 'data')
        self.sample_g1_log = os.path.join(self.test_data_dir, 'sample_g1.log')
        self.sample_j9_log = os.path.join(self.test_data_dir, 'sample_j9.log')
    
    async def test_complete_workflow(self):
        """æµ‹è¯•å®Œæ•´çš„GCåˆ†æå·¥ä½œæµç¨‹"""
        print("ğŸš€ å¼€å§‹å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•...")
        
        if not os.path.exists(self.sample_g1_log):
            print("âš ï¸ è·³è¿‡æµ‹è¯•ï¼šG1æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        try:
            # 1. åˆ†æGCæ—¥å¿—
            print("ğŸ“Š æ­¥éª¤1: åˆ†æG1 GCæ—¥å¿—...")
            analyze_result = await analyze_gc_log_tool({
                "file_path": self.sample_g1_log,
                "analysis_type": "detailed"
            })
            
            print("âœ… æ—¥å¿—åˆ†æå®Œæˆ")
            assert analyze_result.content, "åˆ†æç»“æœåº”è¯¥æœ‰å†…å®¹"
            
            # 2. è·å–è¯¦ç»†æŒ‡æ ‡
            print("ğŸ“ˆ æ­¥éª¤2: è·å–è¯¦ç»†æ€§èƒ½æŒ‡æ ‡...")
            metrics_result = await get_gc_metrics_tool({
                "metric_types": ["all"]
            })
            
            print("âœ… æŒ‡æ ‡è·å–å®Œæˆ")
            assert metrics_result.content, "æŒ‡æ ‡ç»“æœåº”è¯¥æœ‰å†…å®¹"
            
            # 3. æ£€æµ‹æ€§èƒ½é—®é¢˜
            print("ğŸ” æ­¥éª¤3: æ£€æµ‹GCæ€§èƒ½é—®é¢˜...")
            issues_result = await detect_gc_issues_tool({
                "threshold_config": {
                    "max_pause_time": 50,  # è®¾ç½®è¾ƒä½é˜ˆå€¼ä»¥è§¦å‘è­¦æŠ¥
                    "min_throughput": 98
                }
            })
            
            print("âœ… é—®é¢˜æ£€æµ‹å®Œæˆ")
            assert issues_result.content, "é—®é¢˜æ£€æµ‹ç»“æœåº”è¯¥æœ‰å†…å®¹"
            
            # 4. ç”ŸæˆMarkdownæŠ¥å‘Š
            print("ğŸ“ æ­¥éª¤4: ç”ŸæˆMarkdownåˆ†ææŠ¥å‘Š...")
            with tempfile.TemporaryDirectory() as temp_dir:
                md_report_file = os.path.join(temp_dir, "gc_analysis_report.md")
                
                md_report_result = await generate_gc_report_tool({
                    "format_type": "markdown",
                    "output_file": md_report_file,
                    "include_alerts": True
                })
                
                print("âœ… MarkdownæŠ¥å‘Šç”Ÿæˆå®Œæˆ")
                assert md_report_result.content, "æŠ¥å‘Šç”Ÿæˆç»“æœåº”è¯¥æœ‰å†…å®¹"
                assert os.path.exists(md_report_file), "MarkdownæŠ¥å‘Šæ–‡ä»¶åº”è¯¥å­˜åœ¨"
                
                # éªŒè¯æŠ¥å‘Šå†…å®¹
                with open(md_report_file, 'r', encoding='utf-8') as f:
                    report_content = f.read()
                
                assert "# GCæ€§èƒ½åˆ†ææŠ¥å‘Š" in report_content, "æŠ¥å‘Šåº”è¯¥åŒ…å«æ ‡é¢˜"
                assert "G1 GC" in report_content, "æŠ¥å‘Šåº”è¯¥åŒ…å«GCç±»å‹"
                print(f"   MarkdownæŠ¥å‘Šé•¿åº¦: {len(report_content)} å­—ç¬¦")
                
                # 5. ç”ŸæˆHTMLæŠ¥å‘Š
                print("ğŸŒ æ­¥éª¤5: ç”ŸæˆHTMLåˆ†ææŠ¥å‘Š...")
                html_report_file = os.path.join(temp_dir, "gc_analysis_report.html")
                
                html_report_result = await generate_gc_report_tool({
                    "format_type": "html",
                    "output_file": html_report_file,
                    "include_alerts": True
                })
                
                print("âœ… HTMLæŠ¥å‘Šç”Ÿæˆå®Œæˆ")
                assert html_report_result.content, "HTMLæŠ¥å‘Šç”Ÿæˆç»“æœåº”è¯¥æœ‰å†…å®¹"
                assert os.path.exists(html_report_file), "HTMLæŠ¥å‘Šæ–‡ä»¶åº”è¯¥å­˜åœ¨"
                
                # éªŒè¯HTMLæŠ¥å‘Šå†…å®¹
                with open(html_report_file, 'r', encoding='utf-8') as f:
                    html_content = f.read()
                
                assert "<!DOCTYPE html>" in html_content, "HTMLæŠ¥å‘Šåº”è¯¥åŒ…å«HTMLå£°æ˜"
                assert "GCæ€§èƒ½åˆ†ææŠ¥å‘Š" in html_content, "HTMLæŠ¥å‘Šåº”è¯¥åŒ…å«æ ‡é¢˜"
                print(f"   HTMLæŠ¥å‘Šé•¿åº¦: {len(html_content)} å­—ç¬¦")
            
            # 6. æ—¥å¿—æ¯”è¾ƒï¼ˆå¦‚æœJ9æ—¥å¿—å­˜åœ¨ï¼‰
            if os.path.exists(self.sample_j9_log):
                print("âš–ï¸ æ­¥éª¤6: æ¯”è¾ƒä¸åŒGCç±»å‹çš„æ€§èƒ½...")
                compare_result = await compare_gc_logs_tool({
                    "file_path_1": self.sample_g1_log,
                    "file_path_2": self.sample_j9_log
                })
                
                print("âœ… æ—¥å¿—æ¯”è¾ƒå®Œæˆ")
                assert compare_result.content, "æ¯”è¾ƒç»“æœåº”è¯¥æœ‰å†…å®¹"
            else:
                print("âš ï¸ è·³è¿‡æ­¥éª¤6ï¼šJ9æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
            
            return True
            
        except Exception as e:
            print(f"âŒ å·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    async def test_all_tools_available(self):
        """æµ‹è¯•æ‰€æœ‰å·¥å…·éƒ½å¯ç”¨"""
        print("ğŸ”§ æµ‹è¯•å·¥å…·å¯ç”¨æ€§...")
        
        try:
            tools = await list_tools()
            
            expected_tools = [
                "analyze_gc_log",
                "get_gc_metrics",
                "compare_gc_logs", 
                "detect_gc_issues",
                "generate_gc_report"  # æ–°å¢çš„æŠ¥å‘Šç”Ÿæˆå·¥å…·
            ]
            
            tool_names = [tool.name for tool in tools]
            
            for expected_tool in expected_tools:
                assert expected_tool in tool_names, f"ç¼ºå°‘å·¥å…·: {expected_tool}"
                print(f"   âœ… {expected_tool}")
            
            print(f"âœ… æ‰€æœ‰{len(expected_tools)}ä¸ªå·¥å…·éƒ½å¯ç”¨")
            return True
            
        except Exception as e:
            print(f"âŒ å·¥å…·å¯ç”¨æ€§æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†èƒ½åŠ›"""
        print("ğŸ›¡ï¸ æµ‹è¯•é”™è¯¯å¤„ç†èƒ½åŠ›...")
        
        try:
            # æµ‹è¯•ä¸å­˜åœ¨çš„æ–‡ä»¶
            try:
                result = await analyze_gc_log_tool({
                    "file_path": "/nonexistent/file.log",
                    "analysis_type": "basic"
                })
                # å¦‚æœæ²¡æœ‰æŠ›å‡ºå¼‚å¸¸ï¼Œæ£€æŸ¥æ˜¯å¦è¿”å›äº†é”™è¯¯ä¿¡æ¯
                if result.content:
                    content_text = result.content[0].text
                    if "é”™è¯¯" in content_text:
                        print("âœ… é”™è¯¯è¢«æ­£ç¡®å¤„ç†å¹¶è¿”å›é”™è¯¯ä¿¡æ¯")
                        return True
                    else:
                        print("âš ï¸ æœªè¿”å›é”™è¯¯ä¿¡æ¯")
                        return False
                else:
                    print("âš ï¸ è¿”å›ç»“æœä¸ºç©º")
                    return False
            except (FileNotFoundError, ValueError) as fe:
                # è¿™æ˜¯æœŸæœ›çš„è¡Œä¸ºï¼Œè¯´æ˜é”™è¯¯è¢«æ­£ç¡®æ•è·
                print(f"âœ… é”™è¯¯è¢«æ­£ç¡®æ•è·: {type(fe).__name__}")
                return True
            except Exception as ue:
                # å…¶ä»–æœªé¢„æœŸçš„å¼‚å¸¸
                print(f"âš ï¸ æœªé¢„æœŸçš„å¼‚å¸¸: {type(ue).__name__}: {ue}")
                return False
            
        except Exception as e:
            print(f"âŒ é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def test_performance_metrics(self):
        """æµ‹è¯•æ€§èƒ½æŒ‡æ ‡çš„å‡†ç¡®æ€§"""
        print("âš¡ æµ‹è¯•æ€§èƒ½æŒ‡æ ‡å‡†ç¡®æ€§...")
        
        if not os.path.exists(self.sample_g1_log):
            print("âš ï¸ è·³è¿‡æµ‹è¯•ï¼šéœ€è¦æµ‹è¯•æ•°æ®æ–‡ä»¶")
            return True
        
        try:
            # åˆ†ææ—¥å¿—
            await analyze_gc_log_tool({
                "file_path": self.sample_g1_log,
                "analysis_type": "detailed"
            })
            
            # è·å–æŒ‡æ ‡
            result = await get_gc_metrics_tool({
                "metric_types": ["throughput", "latency", "frequency"]
            })
            
            content_text = result.content[0].text
            
            # éªŒè¯å…³é”®æŒ‡æ ‡å­˜åœ¨
            assert "ååé‡æŒ‡æ ‡" in content_text, "åº”è¯¥åŒ…å«ååé‡æŒ‡æ ‡"
            assert "å»¶è¿ŸæŒ‡æ ‡" in content_text, "åº”è¯¥åŒ…å«å»¶è¿ŸæŒ‡æ ‡"
            assert "é¢‘ç‡æŒ‡æ ‡" in content_text, "åº”è¯¥åŒ…å«é¢‘ç‡æŒ‡æ ‡"
            
            print("âœ… æ€§èƒ½æŒ‡æ ‡å‡†ç¡®æ€§éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"âŒ æ€§èƒ½æŒ‡æ ‡æµ‹è¯•å¤±è´¥: {e}")
            return False


async def run_integration_tests():
    """è¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•"""
    test_instance = TestEndToEndIntegration()
    test_instance.setup_method()
    
    tests = [
        ("å·¥å…·å¯ç”¨æ€§", test_instance.test_all_tools_available),
        ("é”™è¯¯å¤„ç†", test_instance.test_error_handling),
        ("æ€§èƒ½æŒ‡æ ‡å‡†ç¡®æ€§", test_instance.test_performance_metrics),
        ("å®Œæ•´å·¥ä½œæµç¨‹", test_instance.test_complete_workflow),
    ]
    
    passed = 0
    total = len(tests)
    
    print("ğŸš€ å¼€å§‹Sprint 5ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•...\n")
    
    for test_name, test_func in tests:
        print(f"ğŸ§ª è¿è¡Œæµ‹è¯•: {test_name}")
        try:
            result = await test_func()
            if result:
                passed += 1
                print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡\n")
            else:
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥\n")
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}\n")
    
    print(f"ğŸ“Š é›†æˆæµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ† GCæ—¥å¿—åˆ†æMCPæœåŠ¡ç³»ç»ŸåŠŸèƒ½å®Œæ•´ä¸”ç¨³å®š")
        return True
    else:
        print("âš ï¸ éƒ¨åˆ†é›†æˆæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥é—®é¢˜")
        return False


if __name__ == "__main__":
    success = asyncio.run(run_integration_tests())
    sys.exit(0 if success else 1)