#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GCæ—¥å¿—åˆ†æWebæœåŠ¡ - å¤§æ–‡ä»¶ä¼˜åŒ–ç‰ˆæœ¬
ä¸“é—¨é’ˆå¯¹6Gå¤§æ–‡ä»¶å¤„ç†è¿›è¡Œä¼˜åŒ–
"""

import os
import sys
import asyncio
import json
import tempfile
import hashlib
from datetime import datetime
from typing import Dict, List, Any, Optional
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

from utils.log_loader import LogLoader, GCLogType
from analyzer.metrics import analyze_gc_metrics
from analyzer.jvm_info_extractor import JVMInfoExtractor
from analyzer.pause_distribution_analyzer import PauseDistributionAnalyzer
from parser.g1_parser import parse_gc_log as parse_g1_log
from parser.ibm_parser import parse_gc_log as parse_j9_log
from rules.alert_engine import GCAlertEngine

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# é…ç½®å¸¸é‡
MAX_FILE_SIZE = 10 * 1024 * 1024 * 1024  # 10GB
CHUNK_SIZE = 16 * 1024 * 1024  # 16MB chunks for optimal performance
SAMPLE_SIZE = 10000  # é‡‡æ ·äº‹ä»¶æ•°é‡
UPLOAD_DIR = "uploads"

os.makedirs(UPLOAD_DIR, exist_ok=True)


class LargeFileOptimizer:
    """å¤§æ–‡ä»¶å¤„ç†ä¼˜åŒ–å™¨ - ä¸“é—¨å¤„ç†6Gçº§åˆ«çš„GCæ—¥å¿—"""
    
    def __init__(self):
        self.loader = LogLoader()
        self.alert_engine = GCAlertEngine()
        self.jvm_extractor = JVMInfoExtractor()
        self.pause_analyzer = PauseDistributionAnalyzer()
        
    async def process_large_gc_log(self, file_path: str, progress_callback=None) -> Dict[str, Any]:
        """
        ä¼˜åŒ–çš„å¤§æ–‡ä»¶å¤„ç†æµç¨‹
        é‡‡ç”¨åˆ†å—è¯»å–ã€æµå¼è§£æã€æ™ºèƒ½é‡‡æ ·ç­‰æŠ€æœ¯
        """
        logger.info(f"å¼€å§‹ä¼˜åŒ–å¤„ç†å¤§æ–‡ä»¶: {file_path}")
        
        file_size = os.path.getsize(file_path)
        logger.info(f"æ–‡ä»¶å¤§å°: {file_size / (1024**3):.2f} GB")
        
        # è¿›åº¦å›è°ƒè¾…åŠ©å‡½æ•°
        def update_progress(stage: str, progress: int, message: str = ""):
            if progress_callback:
                progress_callback(stage, progress, message)
        
        # 1. å¿«é€Ÿç±»å‹æ£€æµ‹ (0-5%)
        update_progress("ç±»å‹æ£€æµ‹", 2, "æ£€æµ‹æ—¥å¿—æ ¼å¼...")
        log_type = await self._detect_type_fast(file_path)
        logger.info(f"æ£€æµ‹åˆ°æ—¥å¿—ç±»å‹: {log_type.value}")
        update_progress("ç±»å‹æ£€æµ‹", 5, f"æ£€æµ‹åˆ° {log_type.value} æ ¼å¼")
        
        # 2. æå–JVMç¯å¢ƒä¿¡æ¯ (5-10%)
        update_progress("ç¯å¢ƒä¿¡æ¯", 7, "æå–JVMç¯å¢ƒä¿¡æ¯...")
        jvm_info = await self._extract_jvm_info(file_path)
        logger.info(f"JVMä¿¡æ¯: {jvm_info.get('jvm_version', 'Unknown')}, {jvm_info.get('gc_strategy', 'Unknown')}")
        update_progress("ç¯å¢ƒä¿¡æ¯", 10, "JVMç¯å¢ƒä¿¡æ¯æå–å®Œæˆ")
        
        # 3. åˆ†å—æµå¼å¤„ç† (10-65%) - è¿™æ˜¯æœ€è€—æ—¶çš„éƒ¨åˆ†
        update_progress("è§£ææ—¥å¿—", 12, "å¼€å§‹è§£æGCäº‹ä»¶...")
        events = await self._stream_parse_file(file_path, log_type, progress_callback)
        logger.info(f"è§£æå®Œæˆï¼Œæ€»äº‹ä»¶æ•°: {len(events)}")
        
        # æ›´æ–°JVMä¿¡æ¯ä¸­çš„è¿è¡Œæ—¶æ•°æ® (65-70%)
        update_progress("è¿è¡Œæ—¶ä¿¡æ¯", 67, "æ›´æ–°è¿è¡Œæ—¶ä¿¡æ¯...")
        if events:
            runtime_info = self.jvm_extractor.extract_jvm_info("", events)
            # åªæ›´æ–°è¿è¡Œæ—¶ç›¸å…³å­—æ®µï¼Œé¿å…è¦†ç›–å·²æ­£ç¡®æå–çš„ç¯å¢ƒä¿¡æ¯
            jvm_info.update({
                'runtime_duration_seconds': runtime_info.get('runtime_duration_seconds', 0),
                'gc_log_start_time': runtime_info.get('gc_log_start_time'),
                'gc_log_end_time': runtime_info.get('gc_log_end_time'),
                'total_gc_events': runtime_info.get('total_gc_events', 0)
            })
        
        # æ·»åŠ å…¼å®¹å‰ç«¯çš„é©¼å³°å‘½åå­—æ®µ - åªæœ‰å½“å€¼ä¸ä¸ºNoneæ—¶æ‰æ·»åŠ 
        if jvm_info.get("total_memory_mb") is not None:
            jvm_info["totalMemoryMb"] = jvm_info["total_memory_mb"]
        if jvm_info.get("maximum_heap_mb") is not None:
            jvm_info["maximumHeapMb"] = jvm_info["maximum_heap_mb"]
        if jvm_info.get("initial_heap_mb") is not None:
            jvm_info["initialHeapMb"] = jvm_info["initial_heap_mb"]
        if jvm_info.get("runtime_duration_seconds") is not None:
            jvm_info["runtimeDurationSeconds"] = jvm_info["runtime_duration_seconds"]
        
        update_progress("è¿è¡Œæ—¶ä¿¡æ¯", 70, "è¿è¡Œæ—¶ä¿¡æ¯æ›´æ–°å®Œæˆ")
        
        # 4. æ™ºèƒ½é‡‡æ ·åˆ†æ (70-75%)
        update_progress("æ•°æ®é‡‡æ ·", 72, "å¼€å§‹æ™ºèƒ½é‡‡æ ·åˆ†æ...")
        sampled_events = self._smart_sample(events)
        logger.info(f"é‡‡æ ·äº‹ä»¶æ•°: {len(sampled_events)}")
        update_progress("æ•°æ®é‡‡æ ·", 75, f"é‡‡æ ·å®Œæˆï¼Œåˆ†æ {len(sampled_events)} ä¸ªå…³é”®äº‹ä»¶")
        
        # 5. æ€§èƒ½æŒ‡æ ‡åˆ†æ (75-82%)
        update_progress("æ€§èƒ½åˆ†æ", 77, "è®¡ç®—æ€§èƒ½æŒ‡æ ‡...")
        metrics = analyze_gc_metrics(sampled_events) if sampled_events else None
        update_progress("æ€§èƒ½åˆ†æ", 82, "æ€§èƒ½æŒ‡æ ‡è®¡ç®—å®Œæˆ")
        
        # 6. åœé¡¿åˆ†å¸ƒåˆ†æ (82-88%)
        update_progress("åœé¡¿åˆ†æ", 84, "åˆ†æåœé¡¿åˆ†å¸ƒ...")
        pause_distribution = self.pause_analyzer.analyze_pause_distribution(sampled_events)
        logger.info(f"åœé¡¿åˆ†å¸ƒåˆ†æå®Œæˆï¼ŒåŒºé—´æ•°: {len(pause_distribution.get('distribution', []))}")
        update_progress("åœé¡¿åˆ†æ", 88, "åœé¡¿åˆ†å¸ƒåˆ†æå®Œæˆ")
        
        # 7. è­¦æŠ¥æ£€æµ‹ (88-93%)
        update_progress("è­¦æŠ¥æ£€æµ‹", 90, "æ£€æµ‹æ€§èƒ½é—®é¢˜...")
        alerts = self.alert_engine.evaluate_metrics(metrics) if metrics else []
        update_progress("è­¦æŠ¥æ£€æµ‹", 93, f"æ£€æµ‹åˆ° {len(alerts)} ä¸ªæ€§èƒ½è­¦æŠ¥")
        
        # 8. ç”Ÿæˆå›¾è¡¨æ•°æ® (93-98%)
        update_progress("å›¾è¡¨ç”Ÿæˆ", 95, "ç”Ÿæˆå›¾è¡¨æ•°æ®...")
        chart_data = self._generate_chart_data(sampled_events, events, pause_distribution)
        update_progress("å›¾è¡¨ç”Ÿæˆ", 98, "å›¾è¡¨æ•°æ®ç”Ÿæˆå®Œæˆ")
        
        # 9. æœ€ç»ˆæ•´ç† (98-100%)
        update_progress("å®Œæˆå¤„ç†", 99, "æ•´ç†åˆ†æç»“æœ...")
        
        result = {
            "log_type": log_type.value,
            "file_size_gb": file_size / (1024**3),
            "total_events": len(events),
            "analyzed_events": len(sampled_events),
            "jvm_info": jvm_info,
            "metrics": self._serialize_metrics(metrics),
            "pause_distribution": pause_distribution,
            "alerts": [self._serialize_alert(a) for a in alerts],
            "chart_data": chart_data,
            "processing_info": {
                "sampling_ratio": len(sampled_events) / len(events) if events else 0,
                "optimization": "large_file_optimized"
            }
        }
        
        update_progress("å®Œæˆå¤„ç†", 100, "åˆ†æå®Œæˆï¼")
        return result
    
    async def _detect_type_fast(self, file_path: str) -> GCLogType:
        """å¿«é€Ÿæ£€æµ‹æ—¥å¿—ç±»å‹ - åªè¯»å–å‰1MB"""
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            sample = f.read(1024 * 1024)
        return self.loader.detect_log_type(sample)
    
    async def _extract_jvm_info(self, file_path: str) -> Dict[str, Any]:
        """æå–JVMç¯å¢ƒä¿¡æ¯ - è¯»å–æ–‡ä»¶å¼€å¤´çš„ç¯å¢ƒä¿¡æ¯"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                # è¯»å–å‰2MBï¼Œé€šå¸¸åŒ…å«æ‰€æœ‰åˆå§‹åŒ–ä¿¡æ¯
                header_content = f.read(2 * 1024 * 1024)
            
            jvm_info = self.jvm_extractor.extract_jvm_info(header_content)
            
            # ä¸è®¾ç½®é»˜è®¤å€¼ï¼Œä¿æŒNoneå€¼ä»¥ä¾¿å‰ç«¯åˆ¤æ–­
            return jvm_info
        except Exception as e:
            logger.warning(f"æå–JVMä¿¡æ¯å¤±è´¥: {e}")
            return {
                'jvm_version': None,
                'gc_strategy': None,
                'cpu_cores': None,
                'total_memory_mb': None,
                'initial_heap_mb': None,
                'maximum_heap_mb': None,
                'runtime_duration_seconds': None,
                'log_format': 'unknown'
            }
    
    async def _stream_parse_file(self, file_path: str, log_type: GCLogType, progress_callback=None) -> List[Dict]:
        """æµå¼è§£æå¤§æ–‡ä»¶"""
        events = []
        total_size = os.path.getsize(file_path)
        processed_size = 0
        
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            buffer = ""
            chunk_count = 0
            
            while True:
                chunk = f.read(CHUNK_SIZE)
                if not chunk:
                    # å¤„ç†æœ€åçš„buffer
                    if buffer:
                        chunk_events = await self._parse_chunk(buffer, log_type)
                        events.extend(chunk_events)
                    break
                
                processed_size += len(chunk.encode('utf-8'))
                chunk_count += 1
                
                # æ·»åŠ åˆ°buffer
                buffer += chunk
                
                # æŸ¥æ‰¾å®Œæ•´çš„æ—¥å¿—è¡Œ
                if log_type == GCLogType.G1:
                    complete_lines, buffer = self._extract_complete_g1_lines(buffer)
                elif log_type == GCLogType.IBM_J9:
                    complete_lines, buffer = self._extract_complete_j9_entries(buffer)
                else:
                    complete_lines, buffer = buffer, ""
                
                if complete_lines:
                    chunk_events = await self._parse_chunk(complete_lines, log_type)
                    events.extend(chunk_events)
                
                # æ›´æ–°è¿›åº¦ - è§£æé˜¶æ®µå 12%-65%çš„è¿›åº¦ï¼Œå…±53%çš„èŒƒå›´
                file_progress = (processed_size / total_size) * 100
                overall_progress = 12 + int(file_progress * 0.53)  # 12% + 53%çš„èŒƒå›´
                
                # æ¯å¤„ç†ä¸€å®šé‡æ•°æ®æ›´æ–°è¿›åº¦
                if chunk_count % 3 == 0:  # æ›´é¢‘ç¹çš„è¿›åº¦æ›´æ–°
                    if progress_callback:
                        progress_callback("è§£ææ—¥å¿—", overall_progress, 
                                        f"å·²å¤„ç† {processed_size/(1024**2):.0f}MB / {total_size/(1024**2):.0f}MBï¼Œè§£æåˆ° {len(events)} ä¸ªäº‹ä»¶")
                    logger.info(f"å¤„ç†è¿›åº¦: {file_progress:.1f}% ({processed_size/(1024**2):.0f}MB)")
                
                # å…è®¸å…¶ä»–ä»»åŠ¡æ‰§è¡Œ
                await asyncio.sleep(0.001)
        
        return events
    
    def _extract_complete_g1_lines(self, buffer: str) -> tuple:
        """æå–å®Œæ•´çš„G1æ—¥å¿—è¡Œ"""
        lines = buffer.split('\n')
        complete_lines = '\n'.join(lines[:-1])
        remaining_buffer = lines[-1]
        return complete_lines, remaining_buffer
    
    def _extract_complete_j9_entries(self, buffer: str) -> tuple:
        """æå–å®Œæ•´çš„J9 XMLæ¡ç›®"""
        # æŸ¥æ‰¾å®Œæ•´çš„<gc>...</gc>æ¡ç›®
        entries = []
        remaining = buffer
        
        while True:
            start = remaining.find('<gc ')
            if start == -1:
                break
            
            end = remaining.find('</gc>', start)
            if end == -1:
                remaining = remaining[start:]
                break
            
            entry = remaining[start:end + 5]
            entries.append(entry)
            remaining = remaining[end + 5:]
        
        return '\n'.join(entries), remaining
    
    async def _parse_chunk(self, chunk: str, log_type: GCLogType) -> List[Dict]:
        """è§£ææ•°æ®å—"""
        try:
            if log_type == GCLogType.G1:
                result = parse_g1_log(chunk)
            elif log_type == GCLogType.IBM_J9:
                result = parse_j9_log(chunk)
            else:
                return []
            
            return result.get('events', [])
        except Exception as e:
            logger.warning(f"è§£æå—å¤±è´¥: {e}")
            return []
    
    def _smart_sample(self, events: List[Dict]) -> List[Dict]:
        """æ™ºèƒ½é‡‡æ · - ä¿ç•™å…³é”®äº‹ä»¶å’Œå‡åŒ€åˆ†å¸ƒ"""
        if len(events) <= SAMPLE_SIZE:
            return events
        
        # åˆ†ç¦»å…³é”®äº‹ä»¶ï¼ˆå¦‚Full GCï¼‰
        critical_events = []
        normal_events = []
        
        for event in events:
            gc_type = event.get('gc_type', '').lower()
            pause_time = event.get('pause_time', 0)
            
            # å…³é”®äº‹ä»¶ï¼šFull GCæˆ–é•¿åœé¡¿
            if 'full' in gc_type or pause_time > 100:
                critical_events.append(event)
            else:
                normal_events.append(event)
        
        # ä¿ç•™æ‰€æœ‰å…³é”®äº‹ä»¶
        sampled = critical_events[:]
        
        # ä»æ™®é€šäº‹ä»¶ä¸­å‡åŒ€é‡‡æ ·
        remaining_slots = SAMPLE_SIZE - len(critical_events)
        if remaining_slots > 0 and normal_events:
            step = max(1, len(normal_events) // remaining_slots)
            sampled.extend(normal_events[::step][:remaining_slots])
        
        # æŒ‰æ—¶é—´æ’åº
        sampled.sort(key=lambda x: x.get('timestamp', 0))
        
        logger.info(f"æ™ºèƒ½é‡‡æ ·: å…³é”®äº‹ä»¶ {len(critical_events)}, æ™®é€šäº‹ä»¶é‡‡æ · {len(sampled) - len(critical_events)}")
        return sampled
    
    def _generate_chart_data(self, sampled_events: List[Dict], all_events: List[Dict], pause_distribution: Optional[Dict] = None) -> Dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–çš„å›¾è¡¨æ•°æ®"""
        # è¿›ä¸€æ­¥é‡‡æ ·ç”¨äºå›¾è¡¨æ˜¾ç¤ºï¼ˆæœ€å¤š1000ä¸ªç‚¹ï¼‰
        chart_events = sampled_events[::max(1, len(sampled_events) // 1000)][:1000]
        
        # æ—¶é—´åºåˆ—æ•°æ® - å¢å¼ºç‰ˆæœ¬ï¼ŒåŒ…å«æ›´å¤šå†…å­˜åŒºåŸŸä¿¡æ¯
        timeline_data = []
        for i, event in enumerate(chart_events):
            # è·å–åŸºæœ¬å†…å­˜ä¿¡æ¯ - å…¼å®¹G1å’ŒJ9æ ¼å¼
            heap_before = event.get('heap_before', 0)
            heap_after = event.get('heap_after', 0)
            heap_total = event.get('heap_total', 0)
            
            # å¤„ç†å†…å­˜å•ä½ï¼ˆå­—èŠ‚è½¬MBï¼‰
            # æ£€æŸ¥æ˜¯å¦ä¸ºå­—èŠ‚å•ä½ï¼ˆé€šå¸¸å¤§äº1MBï¼‰
            if heap_before > 1048576:  # å¦‚æœå¤§äº1MBï¼Œå‡è®¾æ˜¯å­—èŠ‚å•ä½
                heap_before = heap_before / (1024 * 1024)  # è½¬æ¢ä¸ºMB
                heap_after = heap_after / (1024 * 1024) if heap_after else 0
                heap_total = heap_total / (1024 * 1024) if heap_total else 0
            gc_type = event.get('gc_type', 'unknown')
            
            # è·å–åœé¡¿æ—¶é—´ - å…¼å®¹ä¸åŒå­—æ®µå
            pause_time = event.get('pause_time') or event.get('duration', 0)
            
            # å¤„ç†IBM J9VMç‰¹æœ‰çš„å†…å­˜åŒºåŸŸä¿¡æ¯
            nursery_before = event.get('nursery_before', 0)
            nursery_after = event.get('nursery_after', 0)
            tenure_before = event.get('tenure_before', 0)
            tenure_after = event.get('tenure_after', 0)
            
            # å¤„ç†å†…å­˜åŒºåŸŸå•ä½ï¼ˆå­—èŠ‚è½¬MBï¼‰
            if nursery_before and nursery_before > 1048576:  # å¦‚æœå¤§äº1MBï¼Œå‡è®¾æ˜¯å­—èŠ‚å•ä½
                nursery_before = nursery_before / (1024 * 1024)
                nursery_after = nursery_after / (1024 * 1024) if nursery_after else 0
                tenure_before = tenure_before / (1024 * 1024) if tenure_before else 0
                tenure_after = tenure_after / (1024 * 1024) if tenure_after else 0
            
            # å¤„ç†æ—¶é—´æˆ³ - è½¬æ¢ä¸ºæ ¼å¼åŒ–æ—¶é—´å­—ç¬¦ä¸²ï¼Œå»æ‰æ—¶åŒºä¿¡æ¯
            timestamp = event.get('timestamp', i)
            if isinstance(timestamp, str) and 'T' in timestamp:
                # å¦‚æœå·²ç»æ˜¯æ ¼å¼åŒ–æ—¶é—´å­—ç¬¦ä¸²ï¼Œå»æ‰æ—¶åŒºä¿¡æ¯
                # ä¾‹å¦‚ï¼š2025-08-26T15:04:37.088+0800 -> 2025-08-26T15:04:37.088
                if '+' in timestamp:
                    formatted_timestamp = timestamp.split('+')[0]
                elif '-' in timestamp and timestamp.count('-') > 2:  # ç¡®ä¿ä¸æ˜¯æ—¥æœŸä¸­çš„-
                    # å¤„ç†è´Ÿæ—¶åŒºåç§»ï¼Œä¾‹å¦‚ 2025-08-26T15:04:37.088-0500
                    last_dash = timestamp.rfind('-')
                    if last_dash > 10:  # ç¡®ä¿ä¸æ˜¯æ—¥æœŸéƒ¨åˆ†çš„-
                        formatted_timestamp = timestamp[:last_dash]
                    else:
                        formatted_timestamp = timestamp
                else:
                    formatted_timestamp = timestamp
            else:
                # å¦‚æœæ˜¯æ•°å­—æˆ–å…¶ä»–æ ¼å¼ï¼Œè½¬æ¢ä¸ºæ—¶é—´æ ¼å¼
                # å‡è®¾æ˜¯ä»æŸä¸ªåŸºå‡†æ—¶é—´å¼€å§‹çš„ç§’æ•°æˆ–äº‹ä»¶åºå·
                from datetime import datetime, timedelta
                base_time = datetime(2025, 8, 26, 15, 4, 37, 88000)  # 2025-08-26T15:04:37.088
                if isinstance(timestamp, (int, float)):
                    # å¦‚æœæ˜¯æ•°å­—ï¼Œå‡è®¾æ˜¯ç§’æ•°åç§»
                    event_time = base_time + timedelta(seconds=timestamp * 10)  # æ¯10ç§’ä¸€ä¸ªäº‹ä»¶
                else:
                    # å¦‚æœæ˜¯å…¶ä»–æ ¼å¼ï¼Œä½¿ç”¨äº‹ä»¶ç´¢å¼•
                    event_time = base_time + timedelta(seconds=i * 10)
                formatted_timestamp = event_time.strftime("%Y-%m-%dT%H:%M:%S.%f")[:-3]  # æ¯«ç§’ç²¾åº¦
            
            # æ ¹æ®GCç±»å‹ä¼°ç®—Edenã€Survivorã€OldåŒºä½¿ç”¨æƒ…å†µ
            # å¯¹äºIBM J9VMï¼Œä¼˜å…ˆä½¿ç”¨çœŸå®çš„å†…å­˜åŒºåŸŸæ•°æ®
            if nursery_before is not None and nursery_after is not None:
                # J9VMçš„NurseryåŒºç±»ä¼¼äºG1çš„EdenåŒº
                estimated_eden_before = nursery_before
                estimated_eden_after = nursery_after
                estimated_old_before = tenure_before if tenure_before else heap_before * 0.7
                estimated_old_after = tenure_after if tenure_after else heap_after * 0.8
                # å¤„ç†SurvivoråŒºåŸŸ
                survivor_before = event.get('survivor_before')
                if survivor_before and survivor_before > 1048576:  # å­—èŠ‚è½¬MB
                    survivor_before = survivor_before / (1024 * 1024)
                estimated_survivor = survivor_before if survivor_before else heap_before * 0.05
            elif gc_type == 'young' or gc_type == 'scavenge':
                # Young GC/Scavengeä¸»è¦å›æ”¶EdenåŒº
                estimated_eden_before = heap_before * 0.3  # 30%ä¼°ç®—ä¸ºEdenåŒº
                estimated_eden_after = heap_after * 0.1    # GCåEdenåŒºå‡ ä¹ä¸ºç©º
                estimated_survivor = heap_before * 0.05    # 5%ä¼°ç®—ä¸ºSurvivoråŒº
                estimated_old_before = heap_before * 0.65   # 65%ä¼°ç®—ä¸ºè€å¹´ä»£
                estimated_old_after = heap_after * 0.8     # GCåè€å¹´ä»£ç•¥æœ‰å¢åŠ 
            elif gc_type == 'mixed' or gc_type == 'global':
                # Mixed GC/Global GCåŒæ—¶å›æ”¶æ–°ç”Ÿä»£å’Œéƒ¨åˆ†è€å¹´ä»£
                estimated_eden_before = heap_before * 0.25
                estimated_eden_after = heap_after * 0.05
                estimated_survivor = heap_before * 0.08
                estimated_old_before = heap_before * 0.67
                estimated_old_after = heap_after * 0.75
            else:  # full GC æˆ– concurrent GC
                # Full GCå›æ”¶æ‰€æœ‰åŒºåŸŸ
                estimated_eden_before = heap_before * 0.2
                estimated_eden_after = 0  # Full GCåEdenåŒºä¸ºç©º
                estimated_survivor = heap_before * 0.05
                estimated_old_before = heap_before * 0.75
                estimated_old_after = heap_after * 0.9
            
            # å¤„ç†Metaspaceå•ä½è½¬æ¢ï¼ˆKBè½¬MBï¼‰
            metaspace_before = event.get('metaspace_before')
            metaspace_after = event.get('metaspace_after')
            metaspace_total = event.get('metaspace_total')
            
            if metaspace_before:
                metaspace_before = metaspace_before / 1024.0  # KBè½¬MB
            if metaspace_after:
                metaspace_after = metaspace_after / 1024.0
            if metaspace_total:
                metaspace_total = metaspace_total / 1024.0
            
            data_point = {
                "index": i,
                "event_id": event.get('event_id', i),
                "timestamp": formatted_timestamp,  # ä½¿ç”¨æ ¼å¼åŒ–çš„æ—¶é—´å­—ç¬¦ä¸²
                "original_timestamp": event.get('timestamp', i),  # ä¿ç•™åŸå§‹æ—¶é—´æˆ³
                "pause_time": pause_time,  # å…¼å®¹G1å’ŒJ9çš„å­—æ®µå
                "gc_type": gc_type,
                # å †å†…å­˜ä¿¡æ¯
                "heap_before_mb": heap_before,
                "heap_after_mb": heap_after,
                "heap_total_mb": heap_total,
                "heap_utilization": (heap_before / max(heap_total, 1)) * 100 if heap_total > 0 else 0,
                # ä¼°ç®—çš„å†…å­˜åŒºåŸŸä¿¡æ¯
                "eden_before_mb": estimated_eden_before,
                "eden_after_mb": estimated_eden_after,
                "survivor_before_mb": estimated_survivor,
                "survivor_after_mb": estimated_survivor * 0.7,  # ä¼°ç®—survivorä¹Ÿæœ‰éƒ¨åˆ†å›æ”¶
                "old_before_mb": estimated_old_before,
                "old_after_mb": estimated_old_after,
                # Metaspaceä¿¡æ¯ï¼ˆä¼˜å…ˆä½¿ç”¨è§£æå¾—åˆ°çš„çœŸå®æ•°æ®ï¼‰
                "metaspace_before_mb": metaspace_before if metaspace_before else heap_total * 0.05,  # KBè½¬MBæˆ–ä¼°ç®—
                "metaspace_after_mb": metaspace_after if metaspace_after else heap_total * 0.05,   # KBè½¬MBæˆ–ä¼°ç®—
                "metaspace_total_mb": metaspace_total if metaspace_total else heap_total * 0.08,   # KBè½¬MBæˆ–ä¼°ç®—
                # è®¡ç®—å›æ”¶æ•ˆç‡
                "memory_reclaimed_mb": heap_before - heap_after,
                "reclaim_efficiency": ((heap_before - heap_after) / max(heap_before, 1)) * 100 if heap_before > 0 else 0
            }
            timeline_data.append(data_point)
        
        # GCç±»å‹ç»Ÿè®¡
        gc_stats = {}
        for event in sampled_events:
            gc_type = event.get('gc_type', 'unknown')
            gc_stats[gc_type] = gc_stats.get(gc_type, 0) + 1
        
        # åœé¡¿æ—¶é—´åˆ†å¸ƒ - å…¼å®¹G1å’ŒJ9æ ¼å¼
        pause_times = []
        for e in sampled_events:
            pause_time = e.get('pause_time') or e.get('duration', 0)
            pause_times.append(pause_time)
        pause_histogram = self._create_histogram(pause_times, 20)
        
        # å†…å­˜ä½¿ç”¨åˆ†å¸ƒ
        heap_utilizations = []
        memory_reclaim_rates = []
        for event in timeline_data:
            heap_utilizations.append(event['heap_utilization'])
            memory_reclaim_rates.append(event['reclaim_efficiency'])
        
        heap_utilization_histogram = self._create_histogram(heap_utilizations, 15) if heap_utilizations else {"bin_edges": [], "counts": []}
        reclaim_rate_histogram = self._create_histogram(memory_reclaim_rates, 15) if memory_reclaim_rates else {"bin_edges": [], "counts": []}
        
        return {
            "timeline": timeline_data,
            "gc_type_stats": gc_stats,
            "pause_histogram": pause_histogram,
            "pause_distribution": pause_distribution,  # æ–°å¢ï¼šåœé¡¿åˆ†å¸ƒåˆ†æç»“æœ
            "heap_utilization_histogram": heap_utilization_histogram,
            "reclaim_rate_histogram": reclaim_rate_histogram,
            "summary": {
                "total_events": len(all_events),
                "chart_events": len(chart_events),
                "avg_pause": sum(pause_times) / len(pause_times) if pause_times else 0,
                "max_pause": max(pause_times) if pause_times else 0,
                "avg_heap_utilization": sum(heap_utilizations) / len(heap_utilizations) if heap_utilizations else 0,
                "avg_reclaim_rate": sum(memory_reclaim_rates) / len(memory_reclaim_rates) if memory_reclaim_rates else 0
            }
        }
    
    def _create_histogram(self, values: List[float], bins: int) -> Dict[str, List]:
        """åˆ›å»ºç›´æ–¹å›¾æ•°æ®"""
        if not values:
            return {"bins": [], "counts": []}
        
        min_val, max_val = min(values), max(values)
        bin_width = (max_val - min_val) / bins if max_val > min_val else 1
        
        bin_edges = [min_val + i * bin_width for i in range(bins + 1)]
        counts = [0] * bins
        
        for val in values:
            bin_idx = min(int((val - min_val) / bin_width), bins - 1)
            counts[bin_idx] += 1
        
        return {
            "bin_edges": bin_edges,
            "counts": counts
        }
    
    def _serialize_metrics(self, metrics: Any) -> Dict[str, Any]:
        """åºåˆ—åŒ–metricså¯¹è±¡"""
        if not metrics:
            return {}
        
        return {
            "throughput_percentage": getattr(metrics, 'throughput_percentage', 0),
            "avg_pause_time": getattr(metrics, 'avg_pause_time', 0),
            "max_pause_time": getattr(metrics, 'max_pause_time', 0),
            "p99_pause_time": getattr(metrics, 'p99_pause_time', 0),
            "gc_frequency": getattr(metrics, 'gc_frequency', 0),
            "performance_score": getattr(metrics, 'performance_score', 0),
            "health_status": getattr(metrics, 'health_status', 'unknown')
        }
    
    def _serialize_alert(self, alert: Any) -> Dict[str, Any]:
        """åºåˆ—åŒ–alertå¯¹è±¡"""
        return {
            "severity": alert.severity.value,
            "category": alert.category.value,
            "message": alert.message,
            "recommendation": alert.recommendation
        }


# æµ‹è¯•å‡½æ•°
async def test_large_file_processing(test_file_path=None):
    """æµ‹è¯•å¤§æ–‡ä»¶å¤„ç†åŠŸèƒ½"""
    optimizer = LargeFileOptimizer()
    
    # ç¡®å®šæµ‹è¯•æ–‡ä»¶
    if test_file_path and os.path.exists(test_file_path):
        test_file = test_file_path
    elif os.path.exists("test/data/sample_g1.log"):
        test_file = "test/data/sample_g1.log"
    else:
        print("âš ï¸ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    print(f"ğŸ§ª æµ‹è¯•æ–‡ä»¶: {test_file}")
    file_size_mb = os.path.getsize(test_file) / (1024 * 1024)
    print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    import time
    start_time = time.time()
    
    result = await optimizer.process_large_gc_log(test_file)
    
    # è®°å½•ç»“æŸæ—¶é—´
    end_time = time.time()
    processing_time = end_time - start_time
    
    print(f"âœ… å¤„ç†å®Œæˆ (è€—æ—¶: {processing_time:.2f}ç§’):")
    print(f"   æ—¥å¿—ç±»å‹: {result['log_type']}")
    print(f"   æ€»äº‹ä»¶æ•°: {result['total_events']:,}")
    print(f"   åˆ†æäº‹ä»¶æ•°: {result['analyzed_events']:,}")
    print(f"   é‡‡æ ·æ¯”ä¾‹: {result['processing_info']['sampling_ratio']:.2%}")
    print(f"   å¤„ç†é€Ÿåº¦: {file_size_mb/processing_time:.1f} MB/ç§’")
    
    if result['metrics']:
        metrics = result['metrics']
        print(f"   æ€§èƒ½è¯„åˆ†: {metrics.get('performance_score', 0):.1f}/100")
        print(f"   å¹³å‡åœé¡¿: {metrics.get('avg_pause_time', 0):.1f}ms")
        print(f"   æœ€å¤§åœé¡¿: {metrics.get('max_pause_time', 0):.1f}ms")
    
    print(f"   è­¦æŠ¥æ•°é‡: {len(result['alerts'])}")
    
    # æ˜¾ç¤ºä¸€äº›è­¦æŠ¥ç¤ºä¾‹
    if result['alerts']:
        print(f"\nâš ï¸ è­¦æŠ¥ç¤ºä¾‹:")
        for i, alert in enumerate(result['alerts'][:3]):
            print(f"   {i+1}. [{alert['severity']}] {alert['message']}")


if __name__ == "__main__":
    import sys
    test_file = sys.argv[1] if len(sys.argv) > 1 else None
    asyncio.run(test_large_file_processing(test_file))